{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5632797-0363-42d8-a43c-2f982988d942",
   "metadata": {},
   "source": [
    "## Similarity Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b25a562e-838d-4bed-ae52-ec12552b2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7165c2-90ea-4381-9d3d-edf5c2e1a784",
   "metadata": {},
   "source": [
    "In this exercise, you've been provided the title and abstract of 500 recent machine learning research papers posted on arXiv.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d7a796d-eaef-4e97-b787-b4bf5811c73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GoT-R1: Unleashing Reasoning Capability of MLL...</td>\n",
       "      <td>Visual generation models have made remarkable ...</td>\n",
       "      <td>http://arxiv.org/abs/2505.17022v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delving into RL for Image Generation with CoT:...</td>\n",
       "      <td>Recent advancements underscore the significant...</td>\n",
       "      <td>http://arxiv.org/abs/2505.17017v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Interactive Post-Training for Vision-Language-...</td>\n",
       "      <td>We introduce RIPT-VLA, a simple and scalable r...</td>\n",
       "      <td>http://arxiv.org/abs/2505.17016v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When Are Concepts Erased From Diffusion Models?</td>\n",
       "      <td>Concept erasure, the ability to selectively pr...</td>\n",
       "      <td>http://arxiv.org/abs/2505.17013v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Understanding Prompt Tuning and In-Context Lea...</td>\n",
       "      <td>Prompting is one of the main ways to adapt a p...</td>\n",
       "      <td>http://arxiv.org/abs/2505.17010v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  GoT-R1: Unleashing Reasoning Capability of MLL...   \n",
       "1  Delving into RL for Image Generation with CoT:...   \n",
       "2  Interactive Post-Training for Vision-Language-...   \n",
       "3    When Are Concepts Erased From Diffusion Models?   \n",
       "4  Understanding Prompt Tuning and In-Context Lea...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Visual generation models have made remarkable ...   \n",
       "1  Recent advancements underscore the significant...   \n",
       "2  We introduce RIPT-VLA, a simple and scalable r...   \n",
       "3  Concept erasure, the ability to selectively pr...   \n",
       "4  Prompting is one of the main ways to adapt a p...   \n",
       "\n",
       "                                 url  \n",
       "0  http://arxiv.org/abs/2505.17022v1  \n",
       "1  http://arxiv.org/abs/2505.17017v1  \n",
       "2  http://arxiv.org/abs/2505.17016v1  \n",
       "3  http://arxiv.org/abs/2505.17013v1  \n",
       "4  http://arxiv.org/abs/2505.17010v1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = pd.read_csv('../data/arxiv_papers.csv')\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e6836f-5f36-4b60-b21b-b9cbf7e5ac06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning\n",
      "\n",
      "Text: Visual generation models have made remarkable progress in creating realistic\n",
      "images from text prompts, yet struggle with complex prompts that specify\n",
      "multiple objects with precise spatial relationships and attributes. Effective\n",
      "handling of such prompts requires explicit reasoning about the semantic content\n",
      "and spatial layout. We present GoT-R1, a framework that applies reinforcement\n",
      "learning to enhance semantic-spatial reasoning in visual generation. Building\n",
      "upon the Generation Chain-of-Thought approach, GoT-R1 enables models to\n",
      "autonomously discover effective reasoning strategies beyond predefined\n",
      "templates through carefully designed reinforcement learning. To achieve this,\n",
      "we propose a dual-stage multi-dimensional reward framework that leverages MLLMs\n",
      "to evaluate both the reasoning process and final output, enabling effective\n",
      "supervision across the entire generation pipeline. The reward system assesses\n",
      "semantic alignment, spatial accuracy, and visual quality in a unified approach.\n",
      "Experimental results demonstrate significant improvements on T2I-CompBench\n",
      "benchmark, particularly in compositional tasks involving precise spatial\n",
      "relationships and attribute binding. GoT-R1 advances the state-of-the-art in\n",
      "image generation by successfully transferring sophisticated reasoning\n",
      "capabilities to the visual generation domain. To facilitate future research, we\n",
      "make our code and pretrained models publicly available at\n",
      "https://github.com/gogoduan/GoT-R1.\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(f'Title: {articles.loc[i,\"title\"]}\\n')\n",
    "print(f'Text: {articles.loc[i,\"abstract\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996fcf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  GoT-R1: Unleashing Reasoning Capability of MLL...   \n",
      "1  Delving into RL for Image Generation with CoT:...   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  Visual generation models have made remarkable ...   \n",
      "1  Recent advancements underscore the significant...   \n",
      "\n",
      "                                 url  \n",
      "0  http://arxiv.org/abs/2505.17022v1  \n",
      "1  http://arxiv.org/abs/2505.17017v1  \n"
     ]
    }
   ],
   "source": [
    "print(articles.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee63919-08d4-400b-bb35-3e9f1b70088a",
   "metadata": {},
   "source": [
    "Let's try out a variety of ways of vectorizing and searching for semantically-similar papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32294485-637d-4b77-9a9d-2badcdfeb6dd",
   "metadata": {},
   "source": [
    "### Method 1: Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b788ee0-3070-4704-be92-8c442ff4c6d9",
   "metadata": {},
   "source": [
    "Fit a CountVectorizer to the abstracts of the articles with all of the defaults.  Then vectorize the dataset using the fit vectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4cc272-474c-4b48-9e77-5a80cb13534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "abstract_vectors = vectorizer.fit_transform(articles['abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ee08b-50fa-468c-9655-47b110fa855b",
   "metadata": {},
   "source": [
    "**Question:** How many dimensions do the embeddings have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03208e89-aa86-4daa-bbd8-5952cfe2df2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dimensions: 7978\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of dimensions: {abstract_vectors.shape[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eecfe606-937c-409e-823a-745fe380d407",
   "metadata": {},
   "source": [
    "Now, let's use the embeddings to look for similar articles to a search query.\n",
    "\n",
    "Apply the vectorizer you fit earlier to this query string to get an embedding. \n",
    "\n",
    "**Hint:** You can't pass a string to a vectorizer, but you can pass a list containing a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f38271b-eb14-4619-8f00-604bcf9bbdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"vector databases for retrieval augmented generation\"\n",
    "\n",
    "query_vector = vectorizer.transform([query])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f2c931-5dcd-49b7-aa8f-b5bdab82a541",
   "metadata": {},
   "source": [
    "Now, we need to find the similarity between our query embedding and each vectorized article.\n",
    "\n",
    "For this, you can use the [cosine similarity function from scikit-learn.](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)\n",
    "\n",
    "Calculate the similarity between the query embedding and each article embedding and save the result to a variable named `similarity_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f558bfc-426f-4940-bebd-ce48398e48f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = cosine_similarity(query_vector, abstract_vectors).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef6eddc-b0a1-4fd0-8381-797313e2bcaa",
   "metadata": {},
   "source": [
    "Now, we need to find the most similar results. To help with this, we can use the [argsort function from numpy](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html), which will give the indices sorted by value. \n",
    "\n",
    "Use the argsort function to find the indices of the 5 most similar articles. Inspect their titles and abstracts. **Warning:** argsort sorts from smallest to largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b3dffd0-5992-4879-a207-08dcc74afca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: MIRB: Mathematical Information Retrieval Benchmark\n",
      "\n",
      "Similarity Score: 0.2698\n",
      "\n",
      "Abstract: Mathematical Information Retrieval (MIR) is the task of retrieving\n",
      "information from mathematical documents and plays a key role in various\n",
      "applications, including theorem search in mathematical libraries, answer\n",
      "retrieval on math forums, and premise selection in automated theorem proving.\n",
      "However, a unified benchmark for evaluating these diverse retrieval tasks has\n",
      "been lacking. In this paper, we introduce MIRB (Mathematical Information\n",
      "Retrieval Benchmark) to assess the MIR capabilities of retrieval models. MIRB\n",
      "includes four tasks: semantic statement retrieval, question-answer retrieval,\n",
      "premise retrieval, and formula retrieval, spanning a total of 12 datasets. We\n",
      "evaluate 13 retrieval models on this benchmark and analyze the challenges\n",
      "inherent to MIR. We hope that MIRB provides a comprehensive framework for\n",
      "evaluating MIR systems and helps advance the development of more effective\n",
      "retrieval models tailored to the mathematical domain....\n",
      "\n",
      "\n",
      "Title: SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval\n",
      "\n",
      "Similarity Score: 0.1893\n",
      "\n",
      "Abstract: Despite the dominance of convolutional and transformer-based architectures in\n",
      "image-to-image retrieval, these models are prone to biases arising from\n",
      "low-level visual features, such as color. Recognizing the lack of semantic\n",
      "understanding as a key limitation, we propose a novel scene graph-based\n",
      "retrieval framework that emphasizes semantic content over superficial image\n",
      "characteristics. Prior approaches to scene graph retrieval predominantly rely\n",
      "on supervised Graph Neural Networks (GNNs), which require ground truth graph\n",
      "pairs driven from image captions. However, the inconsistency of caption-based\n",
      "supervision stemming from variable text encodings undermine retrieval\n",
      "reliability. To address these, we present SCENIR, a Graph Autoencoder-based\n",
      "unsupervised retrieval framework, which eliminates the dependence on labeled\n",
      "training data. Our model demonstrates superior performance across metrics and\n",
      "runtime efficiency, outperforming existing vision-based, multimodal, and\n",
      "supervised GNN approaches. We further advocate for Graph Edit Distance (GED) as\n",
      "a deterministic and robust ground truth measure for scene graph similarity,\n",
      "replacing the inconsistent caption-based alternatives for the first time in\n",
      "image-to-image retrieval evaluation. Finally, we validate the generalizability\n",
      "of our method by applying it to unannotated datasets via automated scene graph\n",
      "generation, while substantially contributing in advancing state-of-the-art in\n",
      "counterfactual image retrieval....\n",
      "\n",
      "\n",
      "Title: WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning\n",
      "\n",
      "Similarity Score: 0.1806\n",
      "\n",
      "Abstract: Tabular data, ubiquitous and rich in informational value, is an increasing\n",
      "focus for deep representation learning, yet progress is hindered by studies\n",
      "centered on single tables or isolated databases, which limits model\n",
      "capabilities due to data scale. While collaborative learning approaches such as\n",
      "federated learning, transfer learning, split learning, and tabular foundation\n",
      "models aim to learn from multiple correlated databases, they are challenged by\n",
      "a scarcity of real-world interconnected tabular resources. Current data lakes\n",
      "and corpora largely consist of isolated databases lacking defined\n",
      "inter-database correlations. To overcome this, we introduce WikiDBGraph, a\n",
      "large-scale graph of 100,000 real-world tabular databases from WikiData,\n",
      "interconnected by 17 million edges and characterized by 13 node and 12 edge\n",
      "properties derived from its database schema and data distribution.\n",
      "WikiDBGraph's weighted edges identify both instance- and feature-overlapped\n",
      "databases. Experiments on these newly identified databases confirm that\n",
      "collaborative learning yields superior performance, thereby offering\n",
      "considerable promise for structured foundation model training while also\n",
      "exposing key challenges and future directions for learning from interconnected\n",
      "tabular data....\n",
      "\n",
      "\n",
      "Title: Explaining Neural Networks with Reasons\n",
      "\n",
      "Similarity Score: 0.1706\n",
      "\n",
      "Abstract: We propose a new interpretability method for neural networks, which is based\n",
      "on a novel mathematico-philosophical theory of reasons. Our method computes a\n",
      "vector for each neuron, called its reasons vector. We then can compute how\n",
      "strongly this reasons vector speaks for various propositions, e.g., the\n",
      "proposition that the input image depicts digit 2 or that the input prompt has a\n",
      "negative sentiment. This yields an interpretation of neurons, and groups\n",
      "thereof, that combines a logical and a Bayesian perspective, and accounts for\n",
      "polysemanticity (i.e., that a single neuron can figure in multiple concepts).\n",
      "We show, both theoretically and empirically, that this method is: (1) grounded\n",
      "in a philosophically established notion of explanation, (2) uniform, i.e.,\n",
      "applies to the common neural network architectures and modalities, (3)\n",
      "scalable, since computing reason vectors only involves forward-passes in the\n",
      "neural network, (4) faithful, i.e., intervening on a neuron based on its reason\n",
      "vector leads to expected changes in model output, (5) correct in that the\n",
      "model's reasons structure matches that of the data source, (6) trainable, i.e.,\n",
      "neural networks can be trained to improve their reason strengths, (7) useful,\n",
      "i.e., it delivers on the needs for interpretability by increasing, e.g.,\n",
      "robustness and fairness....\n",
      "\n",
      "\n",
      "Title: Scan, Materialize, Simulate: A Generalizable Framework for Physically Grounded Robot Planning\n",
      "\n",
      "Similarity Score: 0.1568\n",
      "\n",
      "Abstract: Autonomous robots must reason about the physical consequences of their\n",
      "actions to operate effectively in unstructured, real-world environments. We\n",
      "present Scan, Materialize, Simulate (SMS), a unified framework that combines 3D\n",
      "Gaussian Splatting for accurate scene reconstruction, visual foundation models\n",
      "for semantic segmentation, vision-language models for material property\n",
      "inference, and physics simulation for reliable prediction of action outcomes.\n",
      "By integrating these components, SMS enables generalizable physical reasoning\n",
      "and object-centric planning without the need to re-learn foundational physical\n",
      "dynamics. We empirically validate SMS in a billiards-inspired manipulation task\n",
      "and a challenging quadrotor landing scenario, demonstrating robust performance\n",
      "on both simulated domain transfer and real-world experiments. Our results\n",
      "highlight the potential of bridging differentiable rendering for scene\n",
      "reconstruction, foundation models for semantic understanding, and physics-based\n",
      "simulation to achieve physically grounded robot planning across diverse\n",
      "settings....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_indices = np.argsort(similarity_scores)[::-1][:5]\n",
    "\n",
    "titles = []\n",
    "for idx in top_indices:\n",
    "    print(f\"\\nTitle: {articles.loc[idx, 'title']}\")\n",
    "    print(f\"\\nSimilarity Score: {similarity_scores[idx]:.4f}\")\n",
    "    print(f\"\\nAbstract: {articles.loc[idx, 'abstract']}...\\n\")\n",
    "    titles.append(articles.loc[idx, 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94c418-8156-4e76-b41b-26276400a92a",
   "metadata": {},
   "source": [
    "Try using a tfidf vectorizer. How do the results compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c574d71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MIRB: Mathematical Information Retrieval Benchmark',\n",
       " 'SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval',\n",
       " 'WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning',\n",
       " 'Explaining Neural Networks with Reasons',\n",
       " 'Scan, Materialize, Simulate: A Generalizable Framework for Physically Grounded Robot Planning']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c5612d4-29a4-4f08-bbeb-efb4ed1f090d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: MIRB: Mathematical Information Retrieval Benchmark\n",
      "\n",
      "Similarity Score (TF-IDF): 0.2996\n",
      "\n",
      "Abstract: Mathematical Information Retrieval (MIR) is the task of retrieving\n",
      "information from mathematical documents and plays a key role in various\n",
      "applications, including theorem search in mathematical libraries, answer\n",
      "retrieval on math forums, and premise selection in automated theorem proving.\n",
      "However, a unified benchmark for evaluating these diverse retrieval tasks has\n",
      "been lacking. In this paper, we introduce MIRB (Mathematical Information\n",
      "Retrieval Benchmark) to assess the MIR capabilities of retrieval models. MIRB\n",
      "includes four tasks: semantic statement retrieval, question-answer retrieval,\n",
      "premise retrieval, and formula retrieval, spanning a total of 12 datasets. We\n",
      "evaluate 13 retrieval models on this benchmark and analyze the challenges\n",
      "inherent to MIR. We hope that MIRB provides a comprehensive framework for\n",
      "evaluating MIR systems and helps advance the development of more effective\n",
      "retrieval models tailored to the mathematical domain....\n",
      "\n",
      "\n",
      "Title: WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning\n",
      "\n",
      "Similarity Score (TF-IDF): 0.2650\n",
      "\n",
      "Abstract: Tabular data, ubiquitous and rich in informational value, is an increasing\n",
      "focus for deep representation learning, yet progress is hindered by studies\n",
      "centered on single tables or isolated databases, which limits model\n",
      "capabilities due to data scale. While collaborative learning approaches such as\n",
      "federated learning, transfer learning, split learning, and tabular foundation\n",
      "models aim to learn from multiple correlated databases, they are challenged by\n",
      "a scarcity of real-world interconnected tabular resources. Current data lakes\n",
      "and corpora largely consist of isolated databases lacking defined\n",
      "inter-database correlations. To overcome this, we introduce WikiDBGraph, a\n",
      "large-scale graph of 100,000 real-world tabular databases from WikiData,\n",
      "interconnected by 17 million edges and characterized by 13 node and 12 edge\n",
      "properties derived from its database schema and data distribution.\n",
      "WikiDBGraph's weighted edges identify both instance- and feature-overlapped\n",
      "databases. Experiments on these newly identified databases confirm that\n",
      "collaborative learning yields superior performance, thereby offering\n",
      "considerable promise for structured foundation model training while also\n",
      "exposing key challenges and future directions for learning from interconnected\n",
      "tabular data....\n",
      "\n",
      "\n",
      "Title: SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval\n",
      "\n",
      "Similarity Score (TF-IDF): 0.2103\n",
      "\n",
      "Abstract: Despite the dominance of convolutional and transformer-based architectures in\n",
      "image-to-image retrieval, these models are prone to biases arising from\n",
      "low-level visual features, such as color. Recognizing the lack of semantic\n",
      "understanding as a key limitation, we propose a novel scene graph-based\n",
      "retrieval framework that emphasizes semantic content over superficial image\n",
      "characteristics. Prior approaches to scene graph retrieval predominantly rely\n",
      "on supervised Graph Neural Networks (GNNs), which require ground truth graph\n",
      "pairs driven from image captions. However, the inconsistency of caption-based\n",
      "supervision stemming from variable text encodings undermine retrieval\n",
      "reliability. To address these, we present SCENIR, a Graph Autoencoder-based\n",
      "unsupervised retrieval framework, which eliminates the dependence on labeled\n",
      "training data. Our model demonstrates superior performance across metrics and\n",
      "runtime efficiency, outperforming existing vision-based, multimodal, and\n",
      "supervised GNN approaches. We further advocate for Graph Edit Distance (GED) as\n",
      "a deterministic and robust ground truth measure for scene graph similarity,\n",
      "replacing the inconsistent caption-based alternatives for the first time in\n",
      "image-to-image retrieval evaluation. Finally, we validate the generalizability\n",
      "of our method by applying it to unannotated datasets via automated scene graph\n",
      "generation, while substantially contributing in advancing state-of-the-art in\n",
      "counterfactual image retrieval....\n",
      "\n",
      "\n",
      "Title: HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases\n",
      "\n",
      "Similarity Score (TF-IDF): 0.1556\n",
      "\n",
      "Abstract: Large Language Models (LLMs) have demonstrated their potential in hardware\n",
      "design tasks, such as Hardware Description Language (HDL) generation and\n",
      "debugging. Yet, their performance in real-world, repository-level HDL projects\n",
      "with thousands or even tens of thousands of code lines is hindered. To this\n",
      "end, we propose HDLxGraph, a novel framework that integrates Graph Retrieval\n",
      "Augmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph\n",
      "representations by incorporating Abstract Syntax Trees (ASTs) and Data Flow\n",
      "Graphs (DFGs) to capture both code graph view and hardware graph view.\n",
      "HDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the\n",
      "limited recall issues inherent in similarity-based semantic retrieval by\n",
      "incorporating structural information, but also enhances its extensibility to\n",
      "various real-world tasks by a task-specific retrieval finetuning. Additionally,\n",
      "to address the lack of comprehensive HDL search benchmarks, we introduce\n",
      "HDLSearch, a multi-granularity evaluation dataset derived from real-world\n",
      "repository-level projects. Experimental results demonstrate that HDLxGraph\n",
      "significantly improves average search accuracy, debugging efficiency and\n",
      "completion quality by 12.04%, 12.22% and 5.04% compared to similarity-based\n",
      "RAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are\n",
      "available at https://github.com/Nick-Zheng-Q/HDLxGraph....\n",
      "\n",
      "\n",
      "Title: Explaining Neural Networks with Reasons\n",
      "\n",
      "Similarity Score (TF-IDF): 0.1424\n",
      "\n",
      "Abstract: We propose a new interpretability method for neural networks, which is based\n",
      "on a novel mathematico-philosophical theory of reasons. Our method computes a\n",
      "vector for each neuron, called its reasons vector. We then can compute how\n",
      "strongly this reasons vector speaks for various propositions, e.g., the\n",
      "proposition that the input image depicts digit 2 or that the input prompt has a\n",
      "negative sentiment. This yields an interpretation of neurons, and groups\n",
      "thereof, that combines a logical and a Bayesian perspective, and accounts for\n",
      "polysemanticity (i.e., that a single neuron can figure in multiple concepts).\n",
      "We show, both theoretically and empirically, that this method is: (1) grounded\n",
      "in a philosophically established notion of explanation, (2) uniform, i.e.,\n",
      "applies to the common neural network architectures and modalities, (3)\n",
      "scalable, since computing reason vectors only involves forward-passes in the\n",
      "neural network, (4) faithful, i.e., intervening on a neuron based on its reason\n",
      "vector leads to expected changes in model output, (5) correct in that the\n",
      "model's reasons structure matches that of the data source, (6) trainable, i.e.,\n",
      "neural networks can be trained to improve their reason strengths, (7) useful,\n",
      "i.e., it delivers on the needs for interpretability by increasing, e.g.,\n",
      "robustness and fairness....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "abstract_vectors_tfidf = tfidf.fit_transform(articles['abstract'])\n",
    "\n",
    "query_vector_tfidf = tfidf.transform([query])\n",
    "\n",
    "similarity_scores_tfidf = cosine_similarity(query_vector_tfidf, abstract_vectors_tfidf).flatten()\n",
    "\n",
    "top_indices_tfidf = np.argsort(similarity_scores_tfidf)[::-1][:5]\n",
    "titles = []\n",
    "for idx in top_indices_tfidf:\n",
    "    print(f\"\\nTitle: {articles.loc[idx, 'title']}\")\n",
    "    print(f\"\\nSimilarity Score (TF-IDF): {similarity_scores_tfidf[idx]:.4f}\")\n",
    "    print(f\"\\nAbstract: {articles.loc[idx, 'abstract']}...\\n\")\n",
    "    titles.append(articles.loc[idx, 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36229925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MIRB: Mathematical Information Retrieval Benchmark',\n",
       " 'WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning',\n",
       " 'SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval',\n",
       " 'HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases',\n",
       " 'Explaining Neural Networks with Reasons']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed45785-5b64-4234-935a-2bb6e3636d5c",
   "metadata": {},
   "source": [
    "### Method 2: Using a Pretrained Embedding Model\n",
    "\n",
    "Now, let's compare how we do using the [all-MiniLM-L6-v2 embedding model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).\n",
    "\n",
    "This will create a 384-dimensional dense embedding of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c08b104a-2599-4f02-8bd0-147205f3ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "369b82b2-a0a3-44be-b89b-097d72a53521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.76568300e-02  6.34958893e-02  4.87130992e-02  7.93049634e-02\n",
      "   3.74480523e-02  2.65282486e-03  3.93749252e-02 -7.09849223e-03\n",
      "   5.93614690e-02  3.15369889e-02  6.00981005e-02 -5.29052056e-02\n",
      "   4.06067446e-02 -2.59308629e-02  2.98427884e-02  1.12694502e-03\n",
      "   7.35149235e-02 -5.03819697e-02 -1.22386590e-01  2.37028170e-02\n",
      "   2.97265705e-02  4.24769074e-02  2.56337989e-02  1.99519354e-03\n",
      "  -5.69190606e-02 -2.71598026e-02 -3.29035893e-02  6.60248324e-02\n",
      "   1.19007140e-01 -4.58791293e-02 -7.26215094e-02 -3.25839706e-02\n",
      "   5.23413755e-02  4.50553112e-02  8.25296156e-03  3.67023535e-02\n",
      "  -1.39415273e-02  6.53919056e-02 -2.64272653e-02  2.06366734e-04\n",
      "  -1.36643462e-02 -3.62809934e-02 -1.95043907e-02 -2.89738290e-02\n",
      "   3.94270420e-02 -8.84090662e-02  2.62422953e-03  1.36714093e-02\n",
      "   4.83063050e-02 -3.11565585e-02 -1.17329165e-01 -5.11690415e-02\n",
      "  -8.85287598e-02 -2.18961909e-02  1.42986253e-02  4.44168225e-02\n",
      "  -1.34814717e-02  7.43392110e-02  2.66382620e-02 -1.98762268e-02\n",
      "   1.79191064e-02 -1.06052477e-02 -9.04263109e-02  2.13269256e-02\n",
      "   1.41204908e-01 -6.47175312e-03 -1.40382058e-03 -1.53610073e-02\n",
      "  -8.73572305e-02  7.22173750e-02  2.01402958e-02  4.25587110e-02\n",
      "  -3.49014401e-02  3.19616229e-04 -8.02970454e-02 -3.27472240e-02\n",
      "   2.85268240e-02 -5.13658300e-02  1.09389238e-01  8.19328427e-02\n",
      "  -9.84039456e-02 -9.34095085e-02 -1.51292449e-02  4.51248288e-02\n",
      "   4.94171903e-02 -2.51867957e-02  1.57077443e-02 -1.29290730e-01\n",
      "   5.31892618e-03  4.02344717e-03 -2.34572608e-02 -6.72982857e-02\n",
      "   2.92280912e-02 -2.60845125e-02  1.30624864e-02 -3.11663728e-02\n",
      "  -4.82713953e-02 -5.58859371e-02 -3.87505442e-02  1.20010830e-01\n",
      "  -1.03924330e-02  4.89704572e-02  5.53537570e-02  4.49358933e-02\n",
      "  -4.00969945e-03 -1.02959745e-01 -2.92968899e-02 -5.83401807e-02\n",
      "   2.70472709e-02 -2.20168922e-02 -7.22241700e-02 -4.13868986e-02\n",
      "  -1.93298087e-02  2.73327134e-03  2.76991050e-04 -9.67587903e-02\n",
      "  -1.00574747e-01 -1.41923055e-02 -8.07891190e-02  4.53925207e-02\n",
      "   2.45040357e-02  5.97613640e-02 -7.38185644e-02  1.19843520e-02\n",
      "  -6.63403273e-02 -7.69045055e-02  3.85157354e-02 -5.59362146e-33\n",
      "   2.80013718e-02 -5.60784303e-02 -4.86601442e-02  2.15569418e-02\n",
      "   6.01980500e-02 -4.81402948e-02 -3.50247175e-02  1.93313882e-02\n",
      "  -1.75152384e-02 -3.89210619e-02 -3.81066022e-03 -1.70287676e-02\n",
      "   2.82099955e-02  1.28290690e-02  4.71601263e-02  6.21029213e-02\n",
      "  -6.43588305e-02  1.29285663e-01 -1.31230708e-02  5.23069575e-02\n",
      "  -3.73680666e-02  2.89094709e-02 -1.68980863e-02 -2.37331036e-02\n",
      "  -3.33491638e-02 -5.16763665e-02  1.55356806e-02  2.08803043e-02\n",
      "  -1.25371488e-02  4.59579192e-02  3.72720063e-02  2.80566867e-02\n",
      "  -5.90005107e-02 -1.16988774e-02  4.92182076e-02  4.70328368e-02\n",
      "   7.35487342e-02 -3.70530151e-02  3.98457050e-03  1.06412228e-02\n",
      "  -1.61590753e-04 -5.27166091e-02  2.75927391e-02 -3.92921306e-02\n",
      "   8.44717771e-02  4.86860834e-02 -4.85869870e-03  1.79948322e-02\n",
      "  -4.28569019e-02  1.23375505e-02  6.39955280e-03  4.04822677e-02\n",
      "   1.48887718e-02 -1.53941838e-02  7.62948096e-02  2.37044040e-02\n",
      "   4.45237309e-02  5.08195683e-02 -2.31256150e-03 -1.88737027e-02\n",
      "  -1.23335626e-02  4.66002263e-02 -5.63438274e-02  6.29926845e-02\n",
      "  -3.15535143e-02  3.24912369e-02  2.34672986e-02 -6.55437782e-02\n",
      "   2.01709364e-02  2.57082526e-02 -1.23868398e-02 -8.36495869e-03\n",
      "  -6.64377511e-02  9.43073854e-02 -3.57092470e-02 -3.42483260e-02\n",
      "  -6.66353526e-03 -8.01521447e-03 -3.09711359e-02  4.33012620e-02\n",
      "  -8.21397547e-03 -1.50795057e-01  3.07691786e-02  4.00718711e-02\n",
      "  -3.79294232e-02  1.93209585e-03  4.00530472e-02 -8.77074376e-02\n",
      "  -3.68491895e-02  8.57954100e-03 -3.19251493e-02 -1.25257885e-02\n",
      "   7.35538825e-02  1.34742574e-03  2.05919091e-02  2.71098183e-33\n",
      "  -5.18576726e-02  5.78360669e-02 -9.18985531e-02  3.94421741e-02\n",
      "   1.05576530e-01 -1.96912549e-02  6.18402846e-02 -7.63464049e-02\n",
      "   2.40880251e-02  9.40049067e-02 -1.16535515e-01  3.71198840e-02\n",
      "   5.22425175e-02 -3.95853538e-03  5.72214685e-02  5.32851880e-03\n",
      "   1.24016829e-01  1.39022134e-02 -1.10249687e-02  3.56053561e-02\n",
      "  -3.30754295e-02  8.16574320e-02 -1.52003653e-02  6.05585016e-02\n",
      "  -6.01397082e-02  3.26102562e-02 -3.48296724e-02 -1.69881582e-02\n",
      "  -9.74907503e-02 -2.71483324e-02  1.74705684e-03 -7.68982321e-02\n",
      "  -4.31857780e-02 -1.89984981e-02 -2.91661285e-02  5.77488020e-02\n",
      "   2.41822060e-02 -1.16902133e-02 -6.21435270e-02  2.84351520e-02\n",
      "  -2.37532091e-04 -2.51783021e-02  4.39638691e-03  8.12840015e-02\n",
      "   3.64184454e-02 -6.04006201e-02 -3.65518071e-02 -7.93748125e-02\n",
      "  -5.08527830e-03  6.69699535e-02 -1.17784336e-01  3.23743261e-02\n",
      "  -4.71252166e-02 -1.34459836e-02 -9.48445350e-02  8.24950729e-03\n",
      "  -1.06748873e-02 -6.81882426e-02  1.11813727e-03  2.48019733e-02\n",
      "  -6.35889545e-02  2.84493119e-02 -2.61303540e-02  8.58111531e-02\n",
      "   1.14682324e-01 -5.35345636e-02 -5.63588403e-02  4.26008850e-02\n",
      "   1.09454561e-02  2.09578797e-02  1.00131176e-01  3.26051638e-02\n",
      "  -1.84208825e-01 -3.93208489e-02 -6.91454411e-02 -6.38105646e-02\n",
      "  -6.56385794e-02 -6.41255500e-03 -4.79612574e-02 -7.68133104e-02\n",
      "   2.95385011e-02 -2.29948666e-02  4.17037010e-02 -2.50047538e-02\n",
      "  -4.54506371e-03 -4.17137183e-02 -1.32289408e-02 -6.38357028e-02\n",
      "  -2.46474426e-03 -1.37337288e-02  1.68976057e-02 -6.30398169e-02\n",
      "   8.98880288e-02  4.18170877e-02 -1.85687393e-02 -1.80442150e-08\n",
      "  -1.67998020e-02 -3.21578160e-02  6.30384013e-02 -4.13092002e-02\n",
      "   4.44819294e-02  2.02467339e-03  6.29592612e-02 -5.17372740e-03\n",
      "  -1.00443903e-02 -3.05640940e-02  3.52673233e-02  5.58581688e-02\n",
      "  -4.67124954e-02  3.45102251e-02  3.29577886e-02  4.30114679e-02\n",
      "   2.94361562e-02 -3.03164404e-02 -1.71107370e-02  7.37484992e-02\n",
      "  -5.47909662e-02  2.77515575e-02  6.20168773e-03  1.58800781e-02\n",
      "   3.42978612e-02 -5.15753916e-03  2.35079397e-02  7.53135607e-02\n",
      "   1.92843210e-02  3.36196497e-02  5.09103462e-02  1.52497053e-01\n",
      "   1.64207257e-02  2.70528141e-02  3.75162773e-02  2.18553767e-02\n",
      "   5.66333905e-02 -3.95747423e-02  7.12313801e-02 -5.41377515e-02\n",
      "   1.03776157e-03  2.11853366e-02 -3.56309041e-02  1.09016985e-01\n",
      "   2.76527903e-03  3.13996784e-02  1.38419366e-03 -3.45738232e-02\n",
      "  -4.59278114e-02  2.88083516e-02  7.16911675e-03  4.84684594e-02\n",
      "   2.61018872e-02 -9.44072660e-03  2.82169655e-02  3.48723568e-02\n",
      "   3.69098447e-02 -8.58946051e-03 -3.53205837e-02 -2.47856881e-02\n",
      "  -1.91921219e-02  3.80707942e-02  5.99653609e-02 -4.22287025e-02]\n",
      " [ 8.64385739e-02  1.02762647e-01  5.39451698e-03  2.04440230e-03\n",
      "  -9.96337086e-03  2.53854953e-02  4.92875949e-02 -3.06265894e-02\n",
      "   6.87254816e-02  1.01366043e-02  7.75397718e-02 -9.00806859e-02\n",
      "   6.10611262e-03 -5.69898598e-02  1.41714886e-02  2.80491598e-02\n",
      "  -8.68464261e-02  7.64398947e-02 -1.03491262e-01 -6.77438080e-02\n",
      "   6.99947029e-02  8.44251215e-02 -7.24917371e-03  1.04770400e-02\n",
      "   1.34020541e-02  6.77576587e-02 -9.42086354e-02 -3.71690057e-02\n",
      "   5.22617809e-02 -3.10853533e-02 -9.63406488e-02  1.57716908e-02\n",
      "   2.57866867e-02  7.85244927e-02  7.89949372e-02  1.91516690e-02\n",
      "   1.64356660e-02  3.10085621e-03  3.81311700e-02  2.37090997e-02\n",
      "   1.05389422e-02 -4.40644845e-02  4.41738404e-02 -2.58727558e-02\n",
      "   6.15378730e-02 -4.05427888e-02 -8.64140540e-02  3.19722705e-02\n",
      "  -8.90681345e-04 -2.44437065e-02 -9.19721425e-02  2.33939420e-02\n",
      "  -8.30293372e-02  4.41510417e-02 -2.49692965e-02  6.23019971e-02\n",
      "  -1.30347686e-03  7.51395300e-02  2.46385057e-02 -6.47243932e-02\n",
      "  -1.17727846e-01  3.83392088e-02 -9.11767855e-02  6.35446385e-02\n",
      "   7.62739703e-02 -8.80241022e-02  9.54557117e-03 -4.69717681e-02\n",
      "  -8.41740668e-02  3.88823710e-02 -1.14393584e-01  6.28856802e-03\n",
      "  -3.49361598e-02  2.39750501e-02 -3.31316963e-02 -1.57244150e-02\n",
      "  -3.78955714e-02 -8.81247502e-03  7.06118941e-02  3.28066312e-02\n",
      "   2.03671004e-03 -1.12278938e-01  6.79721450e-03  1.22765237e-02\n",
      "   3.35303359e-02 -1.36200609e-02 -2.25490090e-02 -2.25228854e-02\n",
      "  -2.03194078e-02  5.04297651e-02 -7.48653039e-02 -8.22821930e-02\n",
      "   7.65962526e-02  4.93392199e-02 -3.75553295e-02  1.44634508e-02\n",
      "  -5.72457947e-02 -1.79954711e-02  1.09697960e-01  1.19462810e-01\n",
      "   8.09207617e-04  6.17057718e-02  3.26322764e-02 -1.30780101e-01\n",
      "  -1.48636609e-01 -6.16232678e-02  4.33885865e-02  2.67128944e-02\n",
      "   1.39786098e-02 -3.94002460e-02 -2.52711494e-02  3.87743278e-03\n",
      "   3.58664803e-02 -6.15420528e-02  3.76660526e-02  2.67564878e-02\n",
      "  -3.82659100e-02 -3.54793407e-02 -2.39227265e-02  8.67977515e-02\n",
      "  -1.84062794e-02  7.71039203e-02  1.39866315e-03  7.00382888e-02\n",
      "  -4.77878153e-02 -7.89819658e-02  5.10813929e-02 -2.99868315e-33\n",
      "  -3.91646214e-02 -2.56214361e-03  1.65210497e-02  9.48937610e-03\n",
      "  -5.66219129e-02  6.57783225e-02 -4.77002524e-02  1.11661898e-02\n",
      "  -5.73558360e-02 -9.16255731e-03 -2.17521433e-02 -5.59531599e-02\n",
      "  -1.11422874e-02  9.32793692e-02  1.66764930e-02 -1.36723639e-02\n",
      "   4.34388369e-02  1.87244301e-03  7.29945581e-03  5.16331978e-02\n",
      "   4.80608903e-02  1.35341495e-01 -1.71739291e-02 -1.29697816e-02\n",
      "  -7.50109553e-02  2.61107516e-02  2.69801989e-02  7.83062540e-04\n",
      "  -4.87270020e-02  1.17842797e-02 -4.59580496e-02 -4.83213738e-02\n",
      "  -1.95671171e-02  1.93889290e-02  1.98807232e-02  1.67432372e-02\n",
      "   9.87800956e-02 -2.74087880e-02  2.34809052e-02  3.70235811e-03\n",
      "  -6.14514686e-02 -1.21234113e-03 -9.50474665e-03  9.25157219e-03\n",
      "   2.38443520e-02  8.61231908e-02  2.26789657e-02  5.45148912e-04\n",
      "   3.47129926e-02  6.25463389e-03 -6.92770956e-03  3.92400436e-02\n",
      "   1.15675060e-02  3.26279365e-02  6.22155629e-02  2.76114419e-02\n",
      "   1.86883751e-02  3.55805755e-02  4.11796123e-02  1.54782208e-02\n",
      "   4.22691517e-02  3.82248498e-02  1.00312997e-02 -2.83246078e-02\n",
      "   4.47052419e-02 -4.10459116e-02 -4.50551976e-03 -5.44734448e-02\n",
      "   2.62321029e-02  1.79862343e-02 -1.23118833e-01 -4.66951951e-02\n",
      "  -1.35913175e-02  6.46709800e-02  3.57349403e-03 -1.22233890e-02\n",
      "  -1.79381873e-02 -2.55502071e-02  2.37223953e-02  4.08665044e-03\n",
      "  -6.51475638e-02  4.43651900e-02  4.68596481e-02 -3.25174406e-02\n",
      "   4.02270816e-03 -3.97600979e-03  1.11939618e-02 -9.95597914e-02\n",
      "   3.33167911e-02  8.01060498e-02  9.42692384e-02 -6.38294294e-02\n",
      "   3.23151909e-02 -5.13553470e-02 -7.49871973e-03  5.30048219e-34\n",
      "  -4.13194597e-02  9.49646533e-02 -1.06401436e-01  4.96590547e-02\n",
      "  -3.41913551e-02 -3.16746011e-02 -1.71556417e-02  1.70098769e-03\n",
      "   5.79757877e-02 -1.21780683e-03 -1.68536287e-02 -5.16912676e-02\n",
      "   5.52998707e-02 -3.42647545e-02  3.08179054e-02 -3.10481135e-02\n",
      "   9.27532613e-02  3.72663587e-02 -2.37398054e-02  4.45893928e-02\n",
      "   1.46153504e-02  1.16239369e-01 -5.00112548e-02  3.88716534e-02\n",
      "   4.24749870e-03  2.56976373e-02  3.27243656e-02  4.29907441e-02\n",
      "  -1.36144767e-02  2.56122313e-02  1.06262444e-02 -8.46864507e-02\n",
      "  -9.52982083e-02  1.08399890e-01 -7.51600116e-02 -1.37773743e-02\n",
      "   6.37337714e-02 -4.49666381e-03 -3.25321443e-02  6.23613894e-02\n",
      "   3.48052680e-02 -3.54922265e-02 -2.00222395e-02  3.66608463e-02\n",
      "  -2.48837136e-02  1.01819215e-02 -7.01233298e-02 -4.31950949e-02\n",
      "   2.95332298e-02 -2.94959900e-04 -3.45386714e-02  1.46675836e-02\n",
      "  -9.83970016e-02 -4.70488332e-02 -8.85494985e-03 -8.89914334e-02\n",
      "   3.50996070e-02 -1.29602030e-01 -4.98865657e-02 -6.12047315e-02\n",
      "  -5.97797148e-02  9.46319662e-03  4.91217412e-02 -7.75026381e-02\n",
      "   8.09726790e-02 -4.79257405e-02  2.34379107e-03  7.57031143e-02\n",
      "  -2.40175929e-02 -1.52545981e-02  4.86738943e-02 -3.85968722e-02\n",
      "  -7.04831555e-02 -1.20348204e-02 -3.88790332e-02 -7.76016563e-02\n",
      "  -1.07243825e-02  1.04187988e-02 -2.13753991e-02 -9.17386189e-02\n",
      "  -1.11345062e-02 -2.96066366e-02  2.46457830e-02  4.65713395e-03\n",
      "  -1.63450111e-02 -3.95219885e-02  7.73373470e-02 -2.84733456e-02\n",
      "  -3.69937136e-03  8.27665627e-02 -1.10408887e-02  3.13983858e-02\n",
      "   5.35094216e-02  5.75145446e-02 -3.17622200e-02 -1.52911266e-08\n",
      "  -7.99661204e-02 -4.76797186e-02 -8.59788656e-02  5.69616221e-02\n",
      "  -4.08865847e-02  2.23832801e-02 -4.64444468e-03 -3.80130783e-02\n",
      "  -3.10671031e-02 -1.07278638e-02  1.97698511e-02  7.77003495e-03\n",
      "  -6.09474303e-03 -3.86376008e-02  2.80272029e-02  6.78138286e-02\n",
      "  -2.35350709e-02  3.21747772e-02  8.02536961e-03 -2.39107460e-02\n",
      "  -1.22002652e-03  3.14598940e-02 -5.24923839e-02 -8.06811638e-03\n",
      "   3.14771920e-03  5.11496700e-02 -4.44104560e-02  6.36013821e-02\n",
      "   3.85083966e-02  3.30433138e-02 -4.18728543e-03  4.95592840e-02\n",
      "  -5.69604896e-02 -6.49710419e-03 -2.49793325e-02 -1.60866734e-02\n",
      "   6.62288964e-02 -2.06310488e-02  1.08045764e-01  1.68546978e-02\n",
      "   1.43812979e-02 -1.32127106e-02 -1.29387379e-01  6.95216581e-02\n",
      "  -5.55773415e-02 -6.75413832e-02 -5.45817474e-03 -6.13591215e-03\n",
      "   3.90841141e-02 -6.28779680e-02  3.74063402e-02 -1.16570918e-02\n",
      "   1.29150124e-02 -5.52494936e-02  5.16075939e-02 -4.30836668e-03\n",
      "   5.80247529e-02  1.86944902e-02  2.27810163e-02  3.21665667e-02\n",
      "   5.37978560e-02  7.02849403e-02  7.49312043e-02 -8.41775164e-02]]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "embeddings = embedder.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf339d4b-23e6-4034-84bf-7198135f758a",
   "metadata": {},
   "source": [
    "Use this new embedder to vectorize the abstracts and then find the most similar to the query. How do the results compare to the other methods?\n",
    "\n",
    "**Warning:** Creating embeddings for all of the articles may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceb666a0-11b3-42fb-b4d0-2e9b5d83dcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---TOP 5 RESULTS USING SENTENCE TRANSFORMER---\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Title: MIRB: Mathematical Information Retrieval Benchmark\n",
      "\n",
      "Similarity Score (Sentence Transformer): 0.4782\n",
      "\n",
      "Abstract: Mathematical Information Retrieval (MIR) is the task of retrieving\n",
      "information from mathematical documents and plays a key role in various\n",
      "applications, including theorem search in mathematical libraries, answer\n",
      "retrieval on math forums, and premise selection in automated theorem proving.\n",
      "However, a unified benchmark for evaluating these diverse retrieval tasks has\n",
      "been lacking. In this paper, we introduce MIRB (Mathematical Information\n",
      "Retrieval Benchmark) to assess the MIR capabilities of retrieval models. MIRB\n",
      "includes four tasks: semantic statement retrieval, question-answer retrieval,\n",
      "premise retrieval, and formula retrieval, spanning a total of 12 datasets. We\n",
      "evaluate 13 retrieval models on this benchmark and analyze the challenges\n",
      "inherent to MIR. We hope that MIRB provides a comprehensive framework for\n",
      "evaluating MIR systems and helps advance the development of more effective\n",
      "retrieval models tailored to the mathematical domain....\n",
      "\n",
      "\n",
      "Title: Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation\n",
      "\n",
      "Similarity Score (Sentence Transformer): 0.4555\n",
      "\n",
      "Abstract: Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)\n",
      "combined with external contexts to enhance the accuracy and reliability of\n",
      "generated responses. However, reliably attributing generated content to\n",
      "specific context segments, context attribution, remains challenging due to the\n",
      "computationally intensive nature of current methods, which often require\n",
      "extensive fine-tuning or human annotation. In this work, we introduce a novel\n",
      "Jensen-Shannon Divergence driven method to Attribute Response to Context\n",
      "(ARC-JSD), enabling efficient and accurate identification of essential context\n",
      "sentences without additional fine-tuning or surrogate modelling. Evaluations on\n",
      "a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using\n",
      "instruction-tuned LLMs in different scales demonstrate superior accuracy and\n",
      "significant computational efficiency improvements compared to the previous\n",
      "surrogate-based method. Furthermore, our mechanistic analysis reveals specific\n",
      "attention heads and multilayer perceptron (MLP) layers responsible for context\n",
      "attribution, providing valuable insights into the internal workings of RAG\n",
      "models....\n",
      "\n",
      "\n",
      "Title: HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases\n",
      "\n",
      "Similarity Score (Sentence Transformer): 0.4515\n",
      "\n",
      "Abstract: Large Language Models (LLMs) have demonstrated their potential in hardware\n",
      "design tasks, such as Hardware Description Language (HDL) generation and\n",
      "debugging. Yet, their performance in real-world, repository-level HDL projects\n",
      "with thousands or even tens of thousands of code lines is hindered. To this\n",
      "end, we propose HDLxGraph, a novel framework that integrates Graph Retrieval\n",
      "Augmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph\n",
      "representations by incorporating Abstract Syntax Trees (ASTs) and Data Flow\n",
      "Graphs (DFGs) to capture both code graph view and hardware graph view.\n",
      "HDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the\n",
      "limited recall issues inherent in similarity-based semantic retrieval by\n",
      "incorporating structural information, but also enhances its extensibility to\n",
      "various real-world tasks by a task-specific retrieval finetuning. Additionally,\n",
      "to address the lack of comprehensive HDL search benchmarks, we introduce\n",
      "HDLSearch, a multi-granularity evaluation dataset derived from real-world\n",
      "repository-level projects. Experimental results demonstrate that HDLxGraph\n",
      "significantly improves average search accuracy, debugging efficiency and\n",
      "completion quality by 12.04%, 12.22% and 5.04% compared to similarity-based\n",
      "RAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are\n",
      "available at https://github.com/Nick-Zheng-Q/HDLxGraph....\n",
      "\n",
      "\n",
      "Title: The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation\n",
      "\n",
      "Similarity Score (Sentence Transformer): 0.4343\n",
      "\n",
      "Abstract: Large language models are able to exploit in-context learning to access\n",
      "external knowledge beyond their training data through retrieval-augmentation.\n",
      "While promising, its inner workings remain unclear. In this work, we shed light\n",
      "on the mechanism of in-context retrieval augmentation for question answering by\n",
      "viewing a prompt as a composition of informational components. We propose an\n",
      "attribution-based method to identify specialized attention heads, revealing\n",
      "in-context heads that comprehend instructions and retrieve relevant contextual\n",
      "information, and parametric heads that store entities' relational knowledge. To\n",
      "better understand their roles, we extract function vectors and modify their\n",
      "attention weights to show how they can influence the answer generation process.\n",
      "Finally, we leverage the gained insights to trace the sources of knowledge used\n",
      "during inference, paving the way towards more safe and transparent language\n",
      "models....\n",
      "\n",
      "\n",
      "Title: Distance Adaptive Beam Search for Provably Accurate Graph-Based Nearest Neighbor Search\n",
      "\n",
      "Similarity Score (Sentence Transformer): 0.4145\n",
      "\n",
      "Abstract: Nearest neighbor search is central in machine learning, information\n",
      "retrieval, and databases. For high-dimensional datasets, graph-based methods\n",
      "such as HNSW, DiskANN, and NSG have become popular thanks to their empirical\n",
      "accuracy and efficiency. These methods construct a directed graph over the\n",
      "dataset and perform beam search on the graph to find nodes close to a given\n",
      "query. While significant work has focused on practical refinements and\n",
      "theoretical understanding of graph-based methods, many questions remain. We\n",
      "propose a new distance-based termination condition for beam search to replace\n",
      "the commonly used condition based on beam width. We prove that, as long as the\n",
      "search graph is navigable, our resulting Adaptive Beam Search method is\n",
      "guaranteed to approximately solve the nearest-neighbor problem, establishing a\n",
      "connection between navigability and the performance of graph-based search. We\n",
      "also provide extensive experiments on our new termination condition for both\n",
      "navigable graphs and approximately navigable graphs used in practice, such as\n",
      "HNSW and Vamana graphs. We find that Adaptive Beam Search outperforms standard\n",
      "beam search over a range of recall values, data sets, graph constructions, and\n",
      "target number of nearest neighbors. It thus provides a simple and practical way\n",
      "to improve the performance of popular methods....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abstract_embeddings = embedder.encode(articles['abstract'].tolist())\n",
    "\n",
    "query_embedding = embedder.encode([query])\n",
    "\n",
    "similarity_scores_st = cosine_similarity(query_embedding, abstract_embeddings).flatten()\n",
    "\n",
    "top_indices_st = np.argsort(similarity_scores_st)[::-1][:5]\n",
    "titles = []\n",
    "print(\"\\n---TOP 5 RESULTS USING SENTENCE TRANSFORMER---\\n\")\n",
    "print(\"--------------------------------------------------\")\n",
    "for idx in top_indices_st:\n",
    "    print(f\"\\nTitle: {articles.loc[idx, 'title']}\")\n",
    "    print(f\"\\nSimilarity Score (Sentence Transformer): {similarity_scores_st[idx]:.4f}\")\n",
    "    print(f\"\\nAbstract: {articles.loc[idx, 'abstract']}...\\n\")\n",
    "    titles.append(articles.loc[idx, 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9824a693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MIRB: Mathematical Information Retrieval Benchmark',\n",
       " 'Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation',\n",
       " 'HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases',\n",
       " 'The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation',\n",
       " 'Distance Adaptive Beam Search for Provably Accurate Graph-Based Nearest Neighbor Search']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8dec64-2aa7-40af-adfd-230c19671593",
   "metadata": {},
   "source": [
    "### FAISS\n",
    "\n",
    "The [Faiss library](https://faiss.ai/index.html) is a library for efficient similarity search and clustering of dense vectors. It can be used to automate the process of finding the most similar abstracts.\n",
    "\n",
    "If we want to use cosine similarity, we need to use the Inner Product. We also need to normalize our vectors so that they all have length 1.\n",
    "\n",
    "Use the [normalize function](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) to normalize both the abstract vectors and the query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e5225fb-e429-4337-92f4-9cdb2468ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_embeddings = normalize(abstract_embeddings)\n",
    "normalized_query = normalize(query_embedding)\n",
    "\n",
    "dimension = normalized_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393bb61e-471a-4660-99d7-e3e513d34098",
   "metadata": {},
   "source": [
    "Now, create an [IndexFlatIP object](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#summary-of-methods) that has dimensions equal to the dimensionality of your vectors. Then add your normalized abstract vectors.\n",
    "\n",
    "Hint: You can mimic the example [here](https://github.com/facebookresearch/faiss/wiki/Getting-started#building-an-index-and-adding-the-vectors-to-it), but substitute in the IndexFlatIP class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18f7cc7f-8889-4b0c-8bd5-54c760ab4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatIP(dimension) # dot product index\n",
    "\n",
    "index.add(normalized_embeddings.astype('float'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e3f95-6f37-4918-800a-094314b9554a",
   "metadata": {},
   "source": [
    "Finally, use the [search function](https://github.com/facebookresearch/faiss/wiki/Getting-started#searching) on your index object to find the 5 most similar articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f884c016-444b-4714-85fc-07a8cdaf2247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---TOP 5 RESULTS USING SENTENCE FAISS---\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Title: MIRB: Mathematical Information Retrieval Benchmark\n",
      "\n",
      "Similarity Score (FAISS): 0.4782\n",
      "\n",
      "Abstract: Mathematical Information Retrieval (MIR) is the task of retrieving\n",
      "information from mathematical documents and plays a key role in various\n",
      "applications, including theorem search in mathematical libraries, answer\n",
      "retrieval on math forums, and premise selection in automated theorem proving.\n",
      "However, a unified benchmark for evaluating these diverse retrieval tasks has\n",
      "been lacking. In this paper, we introduce MIRB (Mathematical Information\n",
      "Retrieval Benchmark) to assess the MIR capabilities of retrieval models. MIRB\n",
      "includes four tasks: semantic statement retrieval, question-answer retrieval,\n",
      "premise retrieval, and formula retrieval, spanning a total of 12 datasets. We\n",
      "evaluate 13 retrieval models on this benchmark and analyze the challenges\n",
      "inherent to MIR. We hope that MIRB provides a comprehensive framework for\n",
      "evaluating MIR systems and helps advance the development of more effective\n",
      "retrieval models tailored to the mathematical domain....\n",
      "\n",
      "\n",
      "Title: Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation\n",
      "\n",
      "Similarity Score (FAISS): 0.4555\n",
      "\n",
      "Abstract: Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)\n",
      "combined with external contexts to enhance the accuracy and reliability of\n",
      "generated responses. However, reliably attributing generated content to\n",
      "specific context segments, context attribution, remains challenging due to the\n",
      "computationally intensive nature of current methods, which often require\n",
      "extensive fine-tuning or human annotation. In this work, we introduce a novel\n",
      "Jensen-Shannon Divergence driven method to Attribute Response to Context\n",
      "(ARC-JSD), enabling efficient and accurate identification of essential context\n",
      "sentences without additional fine-tuning or surrogate modelling. Evaluations on\n",
      "a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using\n",
      "instruction-tuned LLMs in different scales demonstrate superior accuracy and\n",
      "significant computational efficiency improvements compared to the previous\n",
      "surrogate-based method. Furthermore, our mechanistic analysis reveals specific\n",
      "attention heads and multilayer perceptron (MLP) layers responsible for context\n",
      "attribution, providing valuable insights into the internal workings of RAG\n",
      "models....\n",
      "\n",
      "\n",
      "Title: HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases\n",
      "\n",
      "Similarity Score (FAISS): 0.4515\n",
      "\n",
      "Abstract: Large Language Models (LLMs) have demonstrated their potential in hardware\n",
      "design tasks, such as Hardware Description Language (HDL) generation and\n",
      "debugging. Yet, their performance in real-world, repository-level HDL projects\n",
      "with thousands or even tens of thousands of code lines is hindered. To this\n",
      "end, we propose HDLxGraph, a novel framework that integrates Graph Retrieval\n",
      "Augmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph\n",
      "representations by incorporating Abstract Syntax Trees (ASTs) and Data Flow\n",
      "Graphs (DFGs) to capture both code graph view and hardware graph view.\n",
      "HDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the\n",
      "limited recall issues inherent in similarity-based semantic retrieval by\n",
      "incorporating structural information, but also enhances its extensibility to\n",
      "various real-world tasks by a task-specific retrieval finetuning. Additionally,\n",
      "to address the lack of comprehensive HDL search benchmarks, we introduce\n",
      "HDLSearch, a multi-granularity evaluation dataset derived from real-world\n",
      "repository-level projects. Experimental results demonstrate that HDLxGraph\n",
      "significantly improves average search accuracy, debugging efficiency and\n",
      "completion quality by 12.04%, 12.22% and 5.04% compared to similarity-based\n",
      "RAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are\n",
      "available at https://github.com/Nick-Zheng-Q/HDLxGraph....\n",
      "\n",
      "\n",
      "Title: The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation\n",
      "\n",
      "Similarity Score (FAISS): 0.4343\n",
      "\n",
      "Abstract: Large language models are able to exploit in-context learning to access\n",
      "external knowledge beyond their training data through retrieval-augmentation.\n",
      "While promising, its inner workings remain unclear. In this work, we shed light\n",
      "on the mechanism of in-context retrieval augmentation for question answering by\n",
      "viewing a prompt as a composition of informational components. We propose an\n",
      "attribution-based method to identify specialized attention heads, revealing\n",
      "in-context heads that comprehend instructions and retrieve relevant contextual\n",
      "information, and parametric heads that store entities' relational knowledge. To\n",
      "better understand their roles, we extract function vectors and modify their\n",
      "attention weights to show how they can influence the answer generation process.\n",
      "Finally, we leverage the gained insights to trace the sources of knowledge used\n",
      "during inference, paving the way towards more safe and transparent language\n",
      "models....\n",
      "\n",
      "\n",
      "Title: Distance Adaptive Beam Search for Provably Accurate Graph-Based Nearest Neighbor Search\n",
      "\n",
      "Similarity Score (FAISS): 0.4145\n",
      "\n",
      "Abstract: Nearest neighbor search is central in machine learning, information\n",
      "retrieval, and databases. For high-dimensional datasets, graph-based methods\n",
      "such as HNSW, DiskANN, and NSG have become popular thanks to their empirical\n",
      "accuracy and efficiency. These methods construct a directed graph over the\n",
      "dataset and perform beam search on the graph to find nodes close to a given\n",
      "query. While significant work has focused on practical refinements and\n",
      "theoretical understanding of graph-based methods, many questions remain. We\n",
      "propose a new distance-based termination condition for beam search to replace\n",
      "the commonly used condition based on beam width. We prove that, as long as the\n",
      "search graph is navigable, our resulting Adaptive Beam Search method is\n",
      "guaranteed to approximately solve the nearest-neighbor problem, establishing a\n",
      "connection between navigability and the performance of graph-based search. We\n",
      "also provide extensive experiments on our new termination condition for both\n",
      "navigable graphs and approximately navigable graphs used in practice, such as\n",
      "HNSW and Vamana graphs. We find that Adaptive Beam Search outperforms standard\n",
      "beam search over a range of recall values, data sets, graph constructions, and\n",
      "target number of nearest neighbors. It thus provides a simple and practical way\n",
      "to improve the performance of popular methods....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "distances, indices = index.search(normalized_query.astype('float'), k)\n",
    "\n",
    "titles = []\n",
    "print(\"\\n---TOP 5 RESULTS USING SENTENCE FAISS---\\n\")\n",
    "print(\"--------------------------------------------------\")\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"\\nTitle: {articles.loc[idx, 'title']}\")\n",
    "    print(f\"\\nSimilarity Score (FAISS): {distances[0][i]:.4f}\")\n",
    "    print(f\"\\nAbstract: {articles.loc[idx, 'abstract']}...\\n\")\n",
    "    titles.append(articles.loc[idx, 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e50ca29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MIRB: Mathematical Information Retrieval Benchmark',\n",
       " 'Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation',\n",
       " 'HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases',\n",
       " 'The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation',\n",
       " 'Distance Adaptive Beam Search for Provably Accurate Graph-Based Nearest Neighbor Search']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(titles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
